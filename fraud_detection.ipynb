{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff796757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import EllipticBitcoinDataset\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c145fd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Enable torch to use mps\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04fda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EllipticBitcoinDataset(root='data/Elliptic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c399a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1\n",
      "Number of classes: 2\n",
      "Number of node features: 165\n",
      "Number of edge features: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f'Number of node features: {dataset.num_node_features}')\n",
    "print(f'Number of edge features: {dataset.num_edge_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82813336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[203769, 165], edge_index=[2, 234355], y=[203769], train_mask=[203769], test_mask=[203769])\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e1a952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features shape: torch.Size([203769, 165])\n",
      "Edge index shape: torch.Size([2, 234355])\n",
      "Labels shape: torch.Size([203769])\n",
      "Number of training nodes: 29894\n",
      "Number of test nodes: 16670\n"
     ]
    }
   ],
   "source": [
    "# Print x shape, edge_index shape, y shape\n",
    "print(f'Node features shape: {data.x.shape}')\n",
    "print(f'Edge index shape: {data.edge_index.shape}')\n",
    "print(f'Labels shape: {data.y.shape}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum().item()}')\n",
    "print(f'Number of test nodes: {data.test_mask.sum().item()}')\n",
    "# NOTE: The graph has 203769 nodes and 234355 directed edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10a3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, dropout=0.5, layer_kwargs=None):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = SAGEConv(dataset.num_node_features, hidden_channels, **(layer_kwargs or {}))\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels, **(layer_kwargs or {}))\n",
    "        self.conv3 = SAGEConv(hidden_channels, dataset.num_classes, **(layer_kwargs or {}))\n",
    "        self.dropout = dropout\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "model = Net(hidden_channels=256, dropout=0.3, layer_kwargs={'aggr': 'mean', 'normalize': True}).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=20, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d631e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the dataset is imbalanced, we use a weighted loss function\n",
    "freq = data.y[data.train_mask].bincount()\n",
    "weights = (1.0 / freq.float())\n",
    "loss_fn = torch.nn.NLLLoss(weight=weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5547ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accs, f1s = [], []\n",
    "    for mask in [data.train_mask, data.test_mask]:\n",
    "        # Compute accuracy\n",
    "        true = data.y[mask]\n",
    "        predicted = pred[mask]\n",
    "        correct = predicted == true\n",
    "        accs.append(int(correct.sum()) / int(mask.sum()))\n",
    "\n",
    "        # Compute F1 score\n",
    "        tp = ((predicted == 1) & (true == 1)).sum().item()\n",
    "        tn = ((predicted == 0) & (true == 0)).sum().item()\n",
    "        fp = ((predicted == 1) & (true == 0)).sum().item()\n",
    "        fn = ((predicted == 0) & (true == 1)).sum().item()\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1s.append(f1)\n",
    "    return accs, f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e45c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010\tLoss: 0.3341\tLR: 0.005000\tTrain Acc: 0.9466\tTest Acc: 0.9023\tTrain F1: 0.7483\tTest F1: 0.4609\n",
      "Epoch: 020\tLoss: 0.3046\tLR: 0.005000\tTrain Acc: 0.9510\tTest Acc: 0.9403\tTrain F1: 0.7477\tTest F1: 0.4674\n",
      "Epoch: 030\tLoss: 0.2889\tLR: 0.005000\tTrain Acc: 0.9612\tTest Acc: 0.9446\tTrain F1: 0.8081\tTest F1: 0.4643\n",
      "Epoch: 040\tLoss: 0.2788\tLR: 0.005000\tTrain Acc: 0.9775\tTest Acc: 0.9487\tTrain F1: 0.8999\tTest F1: 0.5406\n",
      "Epoch: 050\tLoss: 0.2711\tLR: 0.005000\tTrain Acc: 0.9815\tTest Acc: 0.9332\tTrain F1: 0.9203\tTest F1: 0.5198\n",
      "Epoch: 060\tLoss: 0.2678\tLR: 0.005000\tTrain Acc: 0.9816\tTest Acc: 0.9371\tTrain F1: 0.9232\tTest F1: 0.5502\n",
      "Epoch: 070\tLoss: 0.2617\tLR: 0.005000\tTrain Acc: 0.9820\tTest Acc: 0.9254\tTrain F1: 0.9256\tTest F1: 0.5028\n",
      "Epoch: 080\tLoss: 0.2619\tLR: 0.005000\tTrain Acc: 0.9801\tTest Acc: 0.9212\tTrain F1: 0.9183\tTest F1: 0.5231\n",
      "Epoch: 090\tLoss: 0.2576\tLR: 0.005000\tTrain Acc: 0.9791\tTest Acc: 0.9306\tTrain F1: 0.9152\tTest F1: 0.5602\n",
      "Epoch: 100\tLoss: 0.2579\tLR: 0.005000\tTrain Acc: 0.9843\tTest Acc: 0.9264\tTrain F1: 0.9344\tTest F1: 0.5413\n",
      "Epoch: 110\tLoss: 0.2539\tLR: 0.005000\tTrain Acc: 0.9847\tTest Acc: 0.9334\tTrain F1: 0.9364\tTest F1: 0.5578\n",
      "Epoch: 120\tLoss: 0.2501\tLR: 0.005000\tTrain Acc: 0.9832\tTest Acc: 0.9269\tTrain F1: 0.9310\tTest F1: 0.5469\n",
      "Epoch: 130\tLoss: 0.2512\tLR: 0.005000\tTrain Acc: 0.9860\tTest Acc: 0.9285\tTrain F1: 0.9419\tTest F1: 0.5429\n",
      "Epoch: 140\tLoss: 0.2512\tLR: 0.005000\tTrain Acc: 0.9853\tTest Acc: 0.9505\tTrain F1: 0.9392\tTest F1: 0.6328\n",
      "Epoch: 150\tLoss: 0.2481\tLR: 0.005000\tTrain Acc: 0.9867\tTest Acc: 0.9553\tTrain F1: 0.9447\tTest F1: 0.6578\n",
      "Epoch: 160\tLoss: 0.2433\tLR: 0.005000\tTrain Acc: 0.9873\tTest Acc: 0.9564\tTrain F1: 0.9471\tTest F1: 0.6566\n",
      "Epoch: 170\tLoss: 0.2451\tLR: 0.005000\tTrain Acc: 0.9880\tTest Acc: 0.9342\tTrain F1: 0.9497\tTest F1: 0.5302\n",
      "Epoch: 180\tLoss: 0.2440\tLR: 0.005000\tTrain Acc: 0.9875\tTest Acc: 0.9578\tTrain F1: 0.9482\tTest F1: 0.6788\n",
      "Epoch: 190\tLoss: 0.2442\tLR: 0.005000\tTrain Acc: 0.9905\tTest Acc: 0.9603\tTrain F1: 0.9600\tTest F1: 0.6577\n",
      "Epoch: 200\tLoss: 0.2412\tLR: 0.002500\tTrain Acc: 0.9907\tTest Acc: 0.9572\tTrain F1: 0.9607\tTest F1: 0.6486\n",
      "Epoch: 210\tLoss: 0.2401\tLR: 0.002500\tTrain Acc: 0.9916\tTest Acc: 0.9526\tTrain F1: 0.9647\tTest F1: 0.6242\n",
      "Epoch: 220\tLoss: 0.2373\tLR: 0.002500\tTrain Acc: 0.9915\tTest Acc: 0.9566\tTrain F1: 0.9643\tTest F1: 0.6444\n",
      "Epoch: 230\tLoss: 0.2365\tLR: 0.002500\tTrain Acc: 0.9933\tTest Acc: 0.9605\tTrain F1: 0.9716\tTest F1: 0.6846\n",
      "Epoch: 240\tLoss: 0.2360\tLR: 0.002500\tTrain Acc: 0.9940\tTest Acc: 0.9540\tTrain F1: 0.9746\tTest F1: 0.6263\n",
      "Epoch: 250\tLoss: 0.2368\tLR: 0.002500\tTrain Acc: 0.9921\tTest Acc: 0.9627\tTrain F1: 0.9667\tTest F1: 0.6872\n",
      "Epoch: 260\tLoss: 0.2372\tLR: 0.002500\tTrain Acc: 0.9914\tTest Acc: 0.9425\tTrain F1: 0.9639\tTest F1: 0.5951\n",
      "Epoch: 270\tLoss: 0.2396\tLR: 0.001250\tTrain Acc: 0.9944\tTest Acc: 0.9529\tTrain F1: 0.9763\tTest F1: 0.6267\n",
      "Epoch: 280\tLoss: 0.2348\tLR: 0.001250\tTrain Acc: 0.9937\tTest Acc: 0.9442\tTrain F1: 0.9734\tTest F1: 0.5984\n",
      "Epoch: 290\tLoss: 0.2341\tLR: 0.001250\tTrain Acc: 0.9944\tTest Acc: 0.9455\tTrain F1: 0.9763\tTest F1: 0.6090\n",
      "Epoch: 300\tLoss: 0.2340\tLR: 0.001250\tTrain Acc: 0.9946\tTest Acc: 0.9544\tTrain F1: 0.9770\tTest F1: 0.6402\n",
      "Epoch: 310\tLoss: 0.2344\tLR: 0.001250\tTrain Acc: 0.9941\tTest Acc: 0.9582\tTrain F1: 0.9750\tTest F1: 0.6717\n",
      "Epoch: 320\tLoss: 0.2342\tLR: 0.001250\tTrain Acc: 0.9946\tTest Acc: 0.9596\tTrain F1: 0.9770\tTest F1: 0.6772\n",
      "Epoch: 330\tLoss: 0.2347\tLR: 0.001250\tTrain Acc: 0.9949\tTest Acc: 0.9478\tTrain F1: 0.9784\tTest F1: 0.6218\n",
      "Epoch: 340\tLoss: 0.2325\tLR: 0.001250\tTrain Acc: 0.9955\tTest Acc: 0.9496\tTrain F1: 0.9805\tTest F1: 0.6155\n",
      "Epoch: 350\tLoss: 0.2323\tLR: 0.001250\tTrain Acc: 0.9947\tTest Acc: 0.9648\tTrain F1: 0.9773\tTest F1: 0.7022\n",
      "Epoch: 360\tLoss: 0.2325\tLR: 0.001250\tTrain Acc: 0.9954\tTest Acc: 0.9636\tTrain F1: 0.9804\tTest F1: 0.6997\n",
      "Epoch: 370\tLoss: 0.2314\tLR: 0.000625\tTrain Acc: 0.9951\tTest Acc: 0.9612\tTrain F1: 0.9793\tTest F1: 0.6779\n",
      "Epoch: 380\tLoss: 0.2319\tLR: 0.000625\tTrain Acc: 0.9958\tTest Acc: 0.9521\tTrain F1: 0.9819\tTest F1: 0.6289\n",
      "Epoch: 390\tLoss: 0.2318\tLR: 0.000625\tTrain Acc: 0.9954\tTest Acc: 0.9593\tTrain F1: 0.9804\tTest F1: 0.6728\n",
      "Epoch: 400\tLoss: 0.2322\tLR: 0.000625\tTrain Acc: 0.9951\tTest Acc: 0.9585\tTrain F1: 0.9790\tTest F1: 0.6698\n",
      "Epoch: 410\tLoss: 0.2309\tLR: 0.000625\tTrain Acc: 0.9958\tTest Acc: 0.9597\tTrain F1: 0.9818\tTest F1: 0.6683\n",
      "Epoch: 420\tLoss: 0.2320\tLR: 0.000625\tTrain Acc: 0.9958\tTest Acc: 0.9646\tTrain F1: 0.9821\tTest F1: 0.6984\n",
      "Epoch: 430\tLoss: 0.2304\tLR: 0.000625\tTrain Acc: 0.9961\tTest Acc: 0.9574\tTrain F1: 0.9831\tTest F1: 0.6603\n",
      "Epoch: 440\tLoss: 0.2302\tLR: 0.000625\tTrain Acc: 0.9960\tTest Acc: 0.9576\tTrain F1: 0.9826\tTest F1: 0.6619\n",
      "Epoch: 450\tLoss: 0.2307\tLR: 0.000313\tTrain Acc: 0.9962\tTest Acc: 0.9600\tTrain F1: 0.9835\tTest F1: 0.6706\n",
      "Epoch: 460\tLoss: 0.2297\tLR: 0.000313\tTrain Acc: 0.9961\tTest Acc: 0.9624\tTrain F1: 0.9832\tTest F1: 0.6861\n",
      "Epoch: 470\tLoss: 0.2296\tLR: 0.000313\tTrain Acc: 0.9961\tTest Acc: 0.9638\tTrain F1: 0.9831\tTest F1: 0.6995\n",
      "Epoch: 480\tLoss: 0.2304\tLR: 0.000313\tTrain Acc: 0.9962\tTest Acc: 0.9649\tTrain F1: 0.9835\tTest F1: 0.7082\n",
      "Epoch: 490\tLoss: 0.2293\tLR: 0.000313\tTrain Acc: 0.9962\tTest Acc: 0.9647\tTrain F1: 0.9835\tTest F1: 0.7051\n",
      "Epoch: 500\tLoss: 0.2303\tLR: 0.000156\tTrain Acc: 0.9962\tTest Acc: 0.9619\tTrain F1: 0.9836\tTest F1: 0.6889\n",
      "Epoch: 510\tLoss: 0.2292\tLR: 0.000156\tTrain Acc: 0.9962\tTest Acc: 0.9623\tTrain F1: 0.9836\tTest F1: 0.6903\n",
      "Epoch: 520\tLoss: 0.2297\tLR: 0.000156\tTrain Acc: 0.9962\tTest Acc: 0.9633\tTrain F1: 0.9838\tTest F1: 0.6946\n",
      "Epoch: 530\tLoss: 0.2295\tLR: 0.000156\tTrain Acc: 0.9962\tTest Acc: 0.9632\tTrain F1: 0.9838\tTest F1: 0.6964\n",
      "Epoch: 540\tLoss: 0.2284\tLR: 0.000078\tTrain Acc: 0.9962\tTest Acc: 0.9630\tTrain F1: 0.9838\tTest F1: 0.6947\n",
      "Epoch: 550\tLoss: 0.2291\tLR: 0.000078\tTrain Acc: 0.9961\tTest Acc: 0.9635\tTrain F1: 0.9834\tTest F1: 0.6978\n",
      "Epoch: 560\tLoss: 0.2289\tLR: 0.000078\tTrain Acc: 0.9961\tTest Acc: 0.9639\tTrain F1: 0.9832\tTest F1: 0.7014\n",
      "Epoch: 570\tLoss: 0.2296\tLR: 0.000039\tTrain Acc: 0.9961\tTest Acc: 0.9639\tTrain F1: 0.9832\tTest F1: 0.7011\n",
      "Epoch: 580\tLoss: 0.2289\tLR: 0.000039\tTrain Acc: 0.9961\tTest Acc: 0.9626\tTrain F1: 0.9834\tTest F1: 0.6933\n",
      "Epoch: 590\tLoss: 0.2288\tLR: 0.000039\tTrain Acc: 0.9962\tTest Acc: 0.9622\tTrain F1: 0.9836\tTest F1: 0.6909\n",
      "Epoch: 600\tLoss: 0.2295\tLR: 0.000020\tTrain Acc: 0.9962\tTest Acc: 0.9623\tTrain F1: 0.9838\tTest F1: 0.6903\n",
      "Epoch: 610\tLoss: 0.2295\tLR: 0.000020\tTrain Acc: 0.9963\tTest Acc: 0.9623\tTrain F1: 0.9839\tTest F1: 0.6897\n",
      "Epoch: 620\tLoss: 0.2283\tLR: 0.000010\tTrain Acc: 0.9963\tTest Acc: 0.9624\tTrain F1: 0.9839\tTest F1: 0.6904\n",
      "Epoch: 630\tLoss: 0.2293\tLR: 0.000010\tTrain Acc: 0.9963\tTest Acc: 0.9626\tTrain F1: 0.9839\tTest F1: 0.6914\n",
      "Epoch: 640\tLoss: 0.2288\tLR: 0.000010\tTrain Acc: 0.9963\tTest Acc: 0.9627\tTrain F1: 0.9841\tTest F1: 0.6921\n",
      "Epoch: 650\tLoss: 0.2297\tLR: 0.000010\tTrain Acc: 0.9963\tTest Acc: 0.9627\tTrain F1: 0.9841\tTest F1: 0.6921\n",
      "Epoch: 660\tLoss: 0.2290\tLR: 0.000005\tTrain Acc: 0.9963\tTest Acc: 0.9627\tTrain F1: 0.9841\tTest F1: 0.6930\n",
      "Epoch: 670\tLoss: 0.2287\tLR: 0.000005\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 680\tLoss: 0.2288\tLR: 0.000002\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 690\tLoss: 0.2290\tLR: 0.000002\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 700\tLoss: 0.2290\tLR: 0.000001\tTrain Acc: 0.9963\tTest Acc: 0.9629\tTrain F1: 0.9841\tTest F1: 0.6937\n",
      "Epoch: 710\tLoss: 0.2286\tLR: 0.000001\tTrain Acc: 0.9963\tTest Acc: 0.9629\tTrain F1: 0.9841\tTest F1: 0.6937\n",
      "Epoch: 720\tLoss: 0.2288\tLR: 0.000001\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 730\tLoss: 0.2295\tLR: 0.000001\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 740\tLoss: 0.2289\tLR: 0.000001\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 750\tLoss: 0.2290\tLR: 0.000000\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 760\tLoss: 0.2290\tLR: 0.000000\tTrain Acc: 0.9963\tTest Acc: 0.9629\tTrain F1: 0.9841\tTest F1: 0.6937\n",
      "Epoch: 770\tLoss: 0.2289\tLR: 0.000000\tTrain Acc: 0.9963\tTest Acc: 0.9628\tTrain F1: 0.9841\tTest F1: 0.6934\n",
      "Epoch: 780\tLoss: 0.2294\tLR: 0.000000\tTrain Acc: 0.9963\tTest Acc: 0.9629\tTrain F1: 0.9841\tTest F1: 0.6937\n",
      "Epoch: 790\tLoss: 0.2291\tLR: 0.000000\tTrain Acc: 0.9963\tTest Acc: 0.9629\tTrain F1: 0.9841\tTest F1: 0.6937\n",
      "Epoch: 800\tLoss: 0.2288\tLR: 0.000000\tTrain Acc: 0.9963\tTest Acc: 0.9629\tTrain F1: 0.9841\tTest F1: 0.6937\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 801):\n",
    "    loss = train()\n",
    "    scheduler.step(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        (train_acc, test_acc), (train_f1, test_f1) = test()\n",
    "        print(f'Epoch: {epoch:03d}\\tLoss: {loss:.4f}\\tLR: {optimizer.param_groups[0][\"lr\"]:.6f}\\t'\n",
    "              f'Train Acc: {train_acc:.4f}\\tTest Acc: {test_acc:.4f}\\t'\n",
    "              f'Train F1: {train_f1:.4f}\\tTest F1: {test_f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
